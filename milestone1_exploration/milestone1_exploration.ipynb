{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7WOHdx1d1Hv"
   },
   "source": [
    "# Milestone 1: Dataset Exploration and Model Setup\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading and analyzing the yahma/alpaca-cleaned dataset\n",
    "2. Setting up the Qwen/Qwen3-0.6B-Base model\n",
    "3. Data preprocessing for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_NW-bBld1Hw"
   },
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xtqVUn_d1Hx"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZGgI7s9d1Hx"
   },
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "joXTOaAtd1Hx",
    "outputId": "c3114ee9-b1c7-41be-8003-4214348a255f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kj877o6Xd1Hx"
   },
   "source": [
    "## 3. Dataset Analysis\n",
    "\n",
    "### 3.1 Load the yahma/alpaca-cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399,
     "referenced_widgets": [
      "e6eb528b121a4323a97de7decc7a1e15",
      "81525a6468f843e7902f1722c3bdcfc9",
      "41bf0b6846694c0f8a94a38054db287a",
      "84f53cdc1bb84ad0bbebb8120041d957",
      "8753762900734b3190fd300bade1b3a0",
      "4aee1f238db84396a83d05b2b6f94576",
      "85bbff33e1d24d789d669192bbc42950",
      "06f0ab1223014506814f078957a91b4d",
      "3c3d9cfdf6ef4feab60f8bdbdc40263a",
      "a74fafac63c44b7bbe4ba3307dc67cb1",
      "39a7a456103d42dd84f7c1fa84ca2088",
      "a05a30fc70444c3cbc438b16a1ffd915",
      "68a76c78d19c4bfbbc4c2dbfe087a385",
      "e3487ad9bc354599b3b3b137e2b13a3b",
      "b3677c9b14b3494695d14ade00f15d01",
      "4bc2d9acfba743c2aac36023f2cce499",
      "7eae808d743244309f9ce57296aef2a4",
      "c239c13c7e954dcbac06fe86beb76c76",
      "1766b01eda3942f6bc4b739fa8de4b45",
      "331f5b51d185450fa1778fca30bfe52a",
      "6a6975a8a5b64d979840559d82528eb8",
      "d6403b1374c7459b9a3cd32f7c9a5f8d",
      "61b450f6b0b54584aa841f660dd4673b",
      "19e4d0b952ba42fe832d94168ac7579c",
      "099aaa3f2d47447fb2c40b14da79194a",
      "5caa6984ec1f409398a671d04bfc3d23",
      "e4aae5121a01414b91e0f94e44218c77",
      "7f16df80489b4d4bb72d8064265d825c",
      "5f7c0bec321445979c0f9541c802c82f",
      "28ae10f99157452595d111578549eb57",
      "b4bb0932377a4bdeae480b886ff13ae4",
      "eaec134f73774bd0960998ac93ed75fc",
      "007f5443e8a944f4a439d5f40b1d308b"
     ]
    },
    "id": "6-NVJyW1d1Hy",
    "outputId": "a9ed9997-6ac6-4658-eeac-63ac42a56d1c"
   },
   "outputs": [],
   "source": [
    "print(\"Loading yahma/alpaca-cleaned dataset...\")\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "print(f\"\\nDataset structure: {dataset}\")\n",
    "print(f\"\\nNumber of examples in train split: {len(dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6DZuoobd1Hy"
   },
   "source": [
    "### 3.2 Examine Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSxSmYfad1Hy",
    "outputId": "b81e0d7d-8ead-47eb-f4c1-260bc890b2b8"
   },
   "outputs": [],
   "source": [
    "# Display the first example to understand the structure\n",
    "print(\"First example from the dataset:\")\n",
    "print(dataset['train'][0])\n",
    "print(f\"\\nDataset features: {dataset['train'].features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HwSkeo9d1Hz"
   },
   "source": [
    "### 3.3 Display 10 Sample Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9BSm24BSd1Hz",
    "outputId": "2eb8aab9-e288-464a-db0d-aa86557a0d0f"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"10 Sample Examples from the Dataset\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(10):\n",
    "    example = dataset['train'][i]\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Instruction: {example['instruction']}\")\n",
    "    print(f\"Input: {example['input']}\")\n",
    "    print(f\"Output: {example['output']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37TnRbJ7d1H0"
   },
   "source": [
    "### 3.4 Statistical Analysis of Instruction and Response Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z5zmCKeKd1H0",
    "outputId": "ca6ed7c4-880b-4bcd-815a-c32d5401e902"
   },
   "outputs": [],
   "source": [
    "# Calculate lengths for all examples\n",
    "instruction_lengths = []\n",
    "input_lengths = []\n",
    "output_lengths = []\n",
    "\n",
    "for example in dataset['train']:\n",
    "    instruction_lengths.append(len(example['instruction'].split()))\n",
    "    input_lengths.append(len(example['input'].split()))\n",
    "    output_lengths.append(len(example['output'].split()))\n",
    "\n",
    "# Calculate statistics\n",
    "def print_statistics(name, lengths):\n",
    "    print(f\"\\n{name} Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(lengths):.2f} words\")\n",
    "    print(f\"  Median: {np.median(lengths):.2f} words\")\n",
    "    print(f\"  Std Dev: {np.std(lengths):.2f} words\")\n",
    "    print(f\"  Min: {np.min(lengths)} words\")\n",
    "    print(f\"  Max: {np.max(lengths)} words\")\n",
    "    print(f\"  25th percentile: {np.percentile(lengths, 25):.2f} words\")\n",
    "    print(f\"  75th percentile: {np.percentile(lengths, 75):.2f} words\")\n",
    "\n",
    "print_statistics(\"Instruction\", instruction_lengths)\n",
    "print_statistics(\"Input\", input_lengths)\n",
    "print_statistics(\"Output (Response)\", output_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTYtfveRd1H0"
   },
   "source": [
    "### 3.5 Visualize Length Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "D_B06QlOd1H0",
    "outputId": "c1f0bd62-a83b-4950-c374-77963966bc5a"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Instruction lengths histogram\n",
    "axes[0].hist(instruction_lengths, bins=50, edgecolor='black')\n",
    "axes[0].set_title('Instruction Lengths Distribution')\n",
    "axes[0].set_xlabel('Length (words)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(np.mean(instruction_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(instruction_lengths):.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Input lengths histogram\n",
    "axes[1].hist(input_lengths, bins=50, edgecolor='black')\n",
    "axes[1].set_title('Input Lengths Distribution')\n",
    "axes[1].set_xlabel('Length (words)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(np.mean(input_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(input_lengths):.2f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Output lengths histogram\n",
    "axes[2].hist(output_lengths, bins=50, edgecolor='black')\n",
    "axes[2].set_title('Output (Response) Lengths Distribution')\n",
    "axes[2].set_xlabel('Length (words)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].axvline(np.mean(output_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(output_lengths):.2f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDW7fsRNd1H0"
   },
   "source": [
    "### 3.6 Create a Subset of 100 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "d64ceaec526846359db610c2c2bc90c2",
      "a2a78ce63a9e4ecbb621fba7463a4dbf",
      "3e2fca5da7474aad91b0cc7f944cdecd",
      "3866314be29b44b9ad45ef65e9fe1776",
      "23ad7a5888d344f4ba54b06ec74cb9e5",
      "cb075b505c3d4c869b2aa70d26da5588",
      "11dbfa0a0b3d47e9bb27af9ac714989b",
      "b78f1fad70c440219111a2b2a8092111",
      "067e173b899942a4888d0c06bb2918e3",
      "d1e1f3425d2440daa64c44149c605a15",
      "f6a516ce0e72409b845dc5160c44fd17"
     ]
    },
    "id": "hraHbThmd1H0",
    "outputId": "35a35e5f-d11e-4c93-ec23-437af9e60cdf"
   },
   "outputs": [],
   "source": [
    "# Create a subset of 100 examples for efficient training\n",
    "subset_dataset = dataset['train'].select(range(100))\n",
    "print(f\"Created subset with {len(subset_dataset)} examples\")\n",
    "\n",
    "# Save the subset for use in Milestone 2\n",
    "subset_dataset.save_to_disk('../milestone2_finetuning/subset_data')\n",
    "print(\"Subset saved to '../milestone2_finetuning/subset_data'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYcSy72Gd1H0"
   },
   "source": [
    "## 4. Model Setup\n",
    "\n",
    "### 4.1 Load Qwen/Qwen3-0.6B-Base Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384,
     "referenced_widgets": [
      "0f883e246213495bb119ceddd5d04560",
      "61d809a030fd41aa8e74efa38ade1ca1",
      "75734d2ccc664e4fb63bfb6c0cef0a77",
      "56d2430d08d54473be1b0249a70cc043",
      "13a943c78c174ae59372564ec3caa95f",
      "7044cce05a744c5a96ee283b259f0d0b",
      "df1be504fa6d47ef870dd91a69caf577",
      "34050ff1b98b429c962cf3be50b69a63",
      "6bf27126ca594f7d89cc918a0f61a70e",
      "56dd426a1ae34f3dbcd304de55531451",
      "af40e20c7b074a258e59c2d102e7e837",
      "373d280864c4451eaf3b44f606d1bb3e",
      "fd5ebf24436243fc83bb195029c230ee",
      "6c21015ea0f54d0fa0d1a1183a63d67d",
      "443a29e51d3a4aceadf28a53da2d80e4",
      "3c615404adc34742a3831a012a30cca8",
      "7eae1c5496264273aaaf753dbecec3e5",
      "0cfeb2081d0d449cabd0b18e979a6435",
      "6cd3d0f6d4b04eab89ef8411f358ed27",
      "31b55feeb37a4501bf48658fbb4468f6",
      "c2835c210f6749bf9811f53b541ab00b",
      "1f931297441d455985d8708f2499814e",
      "73b40274edd14c1385f445cd086a0ef6",
      "2ebedf4defeb4422b109aee99eeea952",
      "0c7befc2aa5f40cfb9a4441fa7860cd9",
      "ec85785cb5414509ba92321b03aaeadd",
      "33e4049783d44ac49256d85ddf669357",
      "6ff317396cc34391b4cec9308bf50715",
      "71d46b9c7a644d3ca2a1fdf7c8d9ccb0",
      "6315c26c67fe4bccacbb387acb4ee5b1",
      "88cd78aa41624e7fbb406dd3824033f7",
      "142499f0c57f4d31a1b5709720caae34",
      "41035471ca12469293d06f895468aa73",
      "87284d967d7e44eba1608ffa0dedb103",
      "4d09c98fcad84ef58e181c82ad902a7a",
      "bc6170be073f40098547db51fed0471b",
      "2baf45420f7645269697f3916df35abc",
      "5fe58f2cb0c748f7921cf7e6b427667a",
      "b459f02365b242e8874e2bc56654a785",
      "2acac5389e4d4ab39428be0127210980",
      "17dfdab23253467e9188ac1eadce8ac5",
      "4d3317894fe3478291e684c83e71a5e8",
      "54c95ab8a6644ad1932fd5c1014e4074",
      "374c22fc5239473ab85943385f7f4ea9",
      "06d976822259492582831c18056f1e39",
      "bbbd6644dd594d78b00c691866d0e1e6",
      "b74b1faf820441a3956455cfdc063db8",
      "dc6addc4bde54b8aae8c1608d2bc8659",
      "07346c6474b64912990ef09e517717b6",
      "f8f03dffa32040b79039af93f7df2f0b",
      "a2d6c78040714c3ba72e73db863fc1a6",
      "c84bc5922ffc4849b0c8fe622e75609d",
      "6f2d7277330e40d19646caa221c135bf",
      "f7fa227826da4781b5142a7b3395f55b",
      "b9be4d1a7a224ec1bfab44ea8f51d2c6",
      "20d410b8071e4f80989e048c7a95a9d1",
      "5e63f2c54b1e44db81c3bb8b5e27282f",
      "d410c48d66744edaa4d89eacb590e50e",
      "477fad1d7e3a4346b42139dc1f259343",
      "433ec807c15a48cbbdbf5194b4ceae78",
      "3f5e65fce95e42ee86f0da61cd10afe3",
      "a7bd78b521dd47fd9834197bb7e3ab3e",
      "5577659323304fc088ad7ccb47336cbb",
      "151fa57668794cd78db3cac7941a7f48",
      "315a7ba7fb0e493da3209c3aa67788ad",
      "8ff980669605424e87d0cf4dd3638db8",
      "7e461429716947da8d93a582a1d654a4",
      "32b74465c8b840929388215709d1676b",
      "64c41620e19e42efb9467c6f83c6805f",
      "0575d90523094409852fb732f552679f",
      "c05d250b4a8a4009b1a44c2800d5698d",
      "61a7f8def9ac4c93a499effeff968e0c",
      "91bc24c825d34059a84a22750ea03426",
      "417fa55c57014066a5848e07d64070ea",
      "866dadb922cc4645b0306cb0a9f05538",
      "781ff30409e1420b8c39d3610b26f811",
      "c4777f6b32754edfa68ca28ecd8894d1"
     ]
    },
    "id": "G1PNUWjFd1H0",
    "outputId": "00c7d0c5-ee6a-4430-bf04-efdd94d656b1"
   },
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B-Base\"\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "print(f\"✓ Tokenizer loaded successfully\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"  Model max length: {tokenizer.model_max_length}\")\n",
    "\n",
    "# Load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "print(f\"✓ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRRhAPvDd1H2"
   },
   "source": [
    "### 4.2 Determine Model Size and Memory Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAH-JJwTd1H2",
    "outputId": "dba90df1-30cc-4630-8132-286770c8fdd1"
   },
   "outputs": [],
   "source": [
    "# Calculate model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Size Information:\")\n",
    "print(f\"  Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
    "\n",
    "# Estimate memory requirements\n",
    "# For float32: 4 bytes per parameter\n",
    "# For float16: 2 bytes per parameter\n",
    "bytes_per_param = 2 if torch.cuda.is_available() else 4\n",
    "memory_bytes = total_params * bytes_per_param\n",
    "memory_gb = memory_bytes / (1024**3)\n",
    "\n",
    "print(f\"\\nEstimated Memory Requirements:\")\n",
    "print(f\"  Model weights: {memory_gb:.2f} GB\")\n",
    "print(f\"  With activations and gradients: ~{memory_gb * 3:.2f} GB (approximate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ntt5bOfPd1H2"
   },
   "source": [
    "### 4.3 Test Base Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yvYHUcw0d1H2",
    "outputId": "aefc5638-a059-48a9-fcba-bcab75e67e9e"
   },
   "outputs": [],
   "source": [
    "# Test generation with base model\n",
    "test_prompt = \"What is the capital of France?\"\n",
    "print(f\"\\nTest Prompt: {test_prompt}\")\n",
    "print(\"\\nGenerating response with base model...\")\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nBase Model Response:\\n{generated_text}\")\n",
    "print(\"\\n✓ Base model generation test successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-dnw13Cd1H2"
   },
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "### 5.1 Format Examples with Alpaca Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-VxbAEQd1H2",
    "outputId": "a3484c9b-7ada-40b3-b35f-8822d6d45849"
   },
   "outputs": [],
   "source": [
    "def format_alpaca_prompt(instruction, input_text, output):\n",
    "    \"\"\"\n",
    "    Format the data according to the Alpaca prompt template.\n",
    "    \"\"\"\n",
    "    if input_text:\n",
    "        prompt = f\"\"\"Below is an instruction that describes a task, possibly with an input, that needs to be completed. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Test the formatting with a few examples\n",
    "print(\"Formatted Example 1:\")\n",
    "print(\"=\" * 80)\n",
    "example = subset_dataset[0]\n",
    "formatted = format_alpaca_prompt(example['instruction'], example['input'], example['output'])\n",
    "print(formatted)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "print(\"\\nFormatted Example 2:\")\n",
    "print(\"=\" * 80)\n",
    "example = subset_dataset[1]\n",
    "formatted = format_alpaca_prompt(example['instruction'], example['input'], example['output'])\n",
    "print(formatted)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTRyua9Nd1H2"
   },
   "source": [
    "### 5.2 Tokenize Formatted Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYZxio7Dd1H3",
    "outputId": "d52898b0-272a-4926-a397-de16fa39ef93"
   },
   "outputs": [],
   "source": [
    "# Tokenize a few examples to demonstrate\n",
    "print(\"Tokenization Demo:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(3):\n",
    "    example = subset_dataset[i]\n",
    "    formatted_text = format_alpaca_prompt(\n",
    "        example['instruction'],\n",
    "        example['input'],\n",
    "        example['output']\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenizer(\n",
    "        formatted_text,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Original text length: {len(formatted_text)} characters\")\n",
    "    print(f\"  Number of tokens: {tokens['input_ids'].shape[1]}\")\n",
    "    print(f\"  Token IDs (first 20): {tokens['input_ids'][0][:20].tolist()}\")\n",
    "\n",
    "    # Decode to verify\n",
    "    decoded = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
    "    print(f\"  Decoded matches original: {decoded[:100] == formatted_text[:100]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Tokenization demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS4VVOkOd1H3"
   },
   "source": [
    "### 5.3 Prepare Full Subset Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152,
     "referenced_widgets": [
      "e2546e50f41249f5859393ccd0f96016",
      "e69f5e1b8b7d4aa58b5ba75f46eb1e24",
      "076102e4b72e425c808d7d33d9c0cb3a",
      "d0390e150b4445478015c5e3cb658293",
      "f30ce49abbc0480e86747574e3b3ecd7",
      "c65e6e0a094f42d69d989d960ebcb9e2",
      "bbaa47310da64a3fa3910e0e34ec4152",
      "d22d630163094b22abfaa1d4ac6bb52b",
      "682cdf8bbe654380968168ae298074d4",
      "ee2918f55c3c4d28a485d89cf638d02c",
      "84c2302ba79f47a0864d850c8e9691c4",
      "9879a8ef45044a66ba8df0f58117072f",
      "3d31c5b07d894e13b6e45000783ce619",
      "0aa538e192544e9e84f55bec9ebc35c4",
      "41016fe403f844c1a817f925158091ec",
      "7093b6b3aab0479c85b381fba63872f7",
      "78df2544da2746c58c549af33ced9c3d",
      "731f98070fe149088e46c00f6515eb2d",
      "48aedc84c4eb4314903dca587d2d5940",
      "addbc58376bc481ea6387b37c62f9e1a",
      "6f0c249b34a64089b6f4e293c4a5eef4",
      "4cb204a532e249a4a3040dd43513859a"
     ]
    },
    "id": "kPPKQPYod1H3",
    "outputId": "c95acf83-7820-448d-92e8-a72ba4b53425"
   },
   "outputs": [],
   "source": [
    "def format_dataset(example):\n",
    "    \"\"\"Format the dataset with Alpaca template.\"\"\"\n",
    "    formatted_text = format_alpaca_prompt(\n",
    "        example['instruction'],\n",
    "        example['input'],\n",
    "        example['output']\n",
    "    )\n",
    "    return {'text': formatted_text}\n",
    "\n",
    "# Apply formatting to the entire subset\n",
    "formatted_subset = subset_dataset.map(format_dataset)\n",
    "\n",
    "print(f\"✓ Formatted {len(formatted_subset)} examples\")\n",
    "print(f\"\\nDataset columns: {formatted_subset.column_names}\")\n",
    "\n",
    "# Save the formatted dataset\n",
    "formatted_subset.save_to_disk('../milestone2_finetuning/formatted_subset_data')\n",
    "print(\"✓ Formatted subset saved to '../milestone2_finetuning/formatted_subset_data'\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
