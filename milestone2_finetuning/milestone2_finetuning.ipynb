{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe-zX5EYnd6c"
      },
      "source": [
        "# Milestone 2: LLM Fine-tuning with LoRA\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Setting up efficient fine-tuning with QLoRA (4-bit quantization)\n",
        "2. Configuring LoRA parameters\n",
        "3. Fine-tuning the Qwen model on the Alpaca subset\n",
        "4. Evaluating the fine-tuned model\n",
        "5. Saving adapter weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMY1ZW2dnd6d"
      },
      "source": [
        "## 1. Environment Setup and Library Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "kk_3ecOjnd6d"
      },
      "outputs": [],
      "source": [
        "# Install required libraries for efficient fine-tuning\n",
        "!pip install -q transformers datasets torch accelerate\n",
        "!pip install -q peft bitsandbytes  # For LoRA and quantization\n",
        "!pip install -q trl  # For training utilities\n",
        "!pip install -q scipy  # Additional dependency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaLfSi_Vnd6e"
      },
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7E1IBwYnd6e",
        "outputId": "d8f616c5-6b69-4a46-b016-3f69e5044a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_from_disk\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel\n",
        ")\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz1v8lkZnd6f"
      },
      "source": [
        "## 3. Load Prepared Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJAwJb39nd6f",
        "outputId": "986b8ef7-f488-4ff9-8344-1eb5837f8363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading formatted dataset...\n",
            "✓ Loaded 100 examples\n",
            "Dataset columns: ['output', 'input', 'instruction', 'text']\n",
            "\n",
            "Sample formatted text:\n",
            "================================================================================\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Give three tips for staying healthy.\n",
            "\n",
            "### Response:\n",
            "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
            "\n",
            "2. Engage in regular physical activity: ...\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Load the formatted dataset from Milestone 1\n",
        "print(\"Loading formatted dataset...\")\n",
        "dataset = load_from_disk('formatted_subset_data')\n",
        "print(f\"✓ Loaded {len(dataset)} examples\")\n",
        "print(f\"Dataset columns: {dataset.column_names}\")\n",
        "\n",
        "# Display a sample\n",
        "print(\"\\nSample formatted text:\")\n",
        "print(\"=\" * 80)\n",
        "print(dataset[0]['text'][:500] + \"...\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0grhkErnd6g"
      },
      "source": [
        "## 4. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBl1tfgnnd6g",
        "outputId": "c077bf48-f29c-491b-8b03-500891ce329d"
      },
      "outputs": [],
      "source": [
        "model_name = \"Qwen/Qwen3-0.6B-Base\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"✓ Tokenizer loaded\")\n",
        "\n",
        "# Load model\n",
        "device_map = \"auto\" if torch.cuda.is_available() else None\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=device_map,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        ")\n",
        "\n",
        "print(f\"✓ Model loaded successfully\")\n",
        "print(f\"  Device map: {device_map}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUe1wELFnd6h"
      },
      "source": [
        "## 5. LoRA Configuration\n",
        "\n",
        "Set up LoRA (Low-Rank Adaptation) parameters for efficient fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zga1mepWnd6h",
        "outputId": "1312caa7-1e1a-4988-85cc-0f50ec6bf1bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LoRA Configuration:\n",
            "  Rank (r): 32\n",
            "  Alpha: 64\n",
            "  Target modules: {'o_proj', 'up_proj', 'q_proj', 'gate_proj', 'v_proj', 'k_proj', 'down_proj'}\n",
            "  Dropout: 0.2\n",
            "  Task type: CAUSAL_LM\n"
          ]
        }
      ],
      "source": [
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=32,                          # LoRA rank - controls the dimensionality of the low-rank matrices\n",
        "    lora_alpha=64,                 # LoRA scaling factor (typically 2*r)\n",
        "    target_modules=[               # Target attention modules for Qwen\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_dropout=0.2,              # Dropout for LoRA layers\n",
        "    bias=\"none\",                   # Don't train bias parameters\n",
        "    task_type=\"CAUSAL_LM\",         # Task type: Causal Language Modeling\n",
        ")\n",
        "\n",
        "print(\"\\nLoRA Configuration:\")\n",
        "print(f\"  Rank (r): {lora_config.r}\")\n",
        "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"  Target modules: {lora_config.target_modules}\")\n",
        "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
        "print(f\"  Task type: {lora_config.task_type}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaL9Sk92nd6h"
      },
      "source": [
        "## 6. Apply LoRA to Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVzcbhKgnd6h",
        "outputId": "5167e7ac-5eb0-43ec-9b5b-4cfb10818575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Parameter Statistics:\n",
            "  Total parameters: 616,235,008 (616.24M)\n",
            "  Trainable parameters: 20,185,088 (20.19M)\n",
            "  Trainable %: 3.28%\n",
            "\n",
            "✓ LoRA adapters applied to model\n"
          ]
        }
      ],
      "source": [
        "# Apply LoRA configuration to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(\"\\nModel Parameter Statistics:\")\n",
        "print(f\"  Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
        "print(f\"  Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
        "print(\"\\n✓ LoRA adapters applied to model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jax3B06dnd6h"
      },
      "source": [
        "## 7. Prepare Dataset for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d09rR5ASnd6h",
        "outputId": "5ff379a8-79e8-4229-c9ba-dd7d13858ba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing dataset...\n",
            "✓ Dataset tokenized\n",
            "  Number of examples: 100\n",
            "  Features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8'))}\n"
          ]
        }
      ],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenize the text examples.\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "# Tokenize the dataset\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names,\n",
        "    desc=\"Tokenizing\"\n",
        ")\n",
        "\n",
        "print(f\"✓ Dataset tokenized\")\n",
        "print(f\"  Number of examples: {len(tokenized_dataset)}\")\n",
        "print(f\"  Features: {tokenized_dataset.features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtcO5c8nnd6h"
      },
      "source": [
        "## 8. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-jRnPe8nd6i",
        "outputId": "926862df-85a5-4aac-8d76-5722cc7f2b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Configuration:\n",
            "  Epochs: 1\n",
            "  Batch size: 4\n",
            "  Gradient accumulation steps: 4\n",
            "  Effective batch size: 16\n",
            "  Learning rate: 2e-05\n",
            "  FP16: True\n",
            "  Optimizer: OptimizerNames.PAGED_ADAMW_8BIT\n"
          ]
        }
      ],
      "source": [
        "# Define training arguments\n",
        "output_dir = \"./results\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,         # Batch size per device\n",
        "    gradient_accumulation_steps=4,         # Accumulate gradients over 4 steps\n",
        "    learning_rate=2e-5,                    # Learning rate\n",
        "    fp16=torch.cuda.is_available(),        # Use mixed precision if CUDA available\n",
        "    logging_steps=5,                       # Log every 5 steps\n",
        "    save_strategy=\"epoch\",                 # Save checkpoint at end of epoch\n",
        "    optim=\"paged_adamw_8bit\" if torch.cuda.is_available() else \"adamw_torch\",\n",
        "    warmup_steps=10,                       # Warmup steps\n",
        "    report_to=\"none\",                      # Don't report to any service\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"  FP16: {training_args.fp16}\")\n",
        "print(f\"  Optimizer: {training_args.optim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn2j3OJEnd6i"
      },
      "source": [
        "## 9. Initialize Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vf_-xxlnd6i",
        "outputId": "0bdfed2f-6552-4c63-94d2-711c7b42ca3d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Trainer initialized\n"
          ]
        }
      ],
      "source": [
        "# Create data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # We're doing causal language modeling, not masked\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"✓ Trainer initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEqmN4U3nd6i"
      },
      "source": [
        "## 10. Fine-tuning Process\n",
        "\n",
        "Train the model for 1 epoch on the 100-example subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "iyzKdBWFnd6i",
        "outputId": "ce47e21a-cf6a-4e7b-b548-59fb3c5e7656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Starting fine-tuning...\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:07, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.460500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "✓ Fine-tuning completed!\n",
            "================================================================================\n",
            "\n",
            "Training Summary:\n",
            "  Duration: 9.72 seconds (0.16 minutes)\n",
            "  Final training loss: 1.5358\n",
            "  Training samples: 100\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Starting fine-tuning...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Train the model\n",
        "train_result = trainer.train()\n",
        "\n",
        "end_time = datetime.now()\n",
        "training_duration = (end_time - start_time).total_seconds()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ Fine-tuning completed!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTraining Summary:\")\n",
        "print(f\"  Duration: {training_duration:.2f} seconds ({training_duration/60:.2f} minutes)\")\n",
        "print(f\"  Final training loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"  Training samples: {len(tokenized_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jVmSuk3nd6i"
      },
      "source": [
        "## 11. Save LoRA Adapter Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR1AqvrVnd6i",
        "outputId": "51f35dfe-98f0-42bf-aea8-1cd3b44f50d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ LoRA adapter weights saved to: ./adapters\n",
            "  Files saved:\n",
            "    - adapter_model.safetensors\n",
            "    - special_tokens_map.json\n",
            "    - added_tokens.json\n",
            "    - adapter_config.json\n",
            "    - tokenizer_config.json\n",
            "    - merges.txt\n",
            "    - chat_template.jinja\n",
            "    - README.md\n",
            "    - vocab.json\n",
            "    - tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "# Save the LoRA adapters\n",
        "adapter_path = \"./adapters\"\n",
        "model.save_pretrained(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "print(f\"✓ LoRA adapter weights saved to: {adapter_path}\")\n",
        "print(f\"  Files saved:\")\n",
        "import os\n",
        "for file in os.listdir(adapter_path):\n",
        "    print(f\"    - {file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JYaHEgand6j"
      },
      "source": [
        "## 12. Save Training Logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyuGDAMQnd6j",
        "outputId": "39daf769-4e2f-459a-a4b9-8a1ad0ed72c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Training logs saved to training_log.txt and training_log.json\n"
          ]
        }
      ],
      "source": [
        "# Create training log\n",
        "training_log = {\n",
        "    \"model_name\": model_name,\n",
        "    \"training_duration_seconds\": training_duration,\n",
        "    \"final_training_loss\": train_result.training_loss,\n",
        "    \"num_epochs\": training_args.num_train_epochs,\n",
        "    \"batch_size\": training_args.per_device_train_batch_size,\n",
        "    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
        "    \"learning_rate\": training_args.learning_rate,\n",
        "    \"lora_rank\": lora_config.r,\n",
        "    \"lora_alpha\": lora_config.lora_alpha,\n",
        "    \"num_training_samples\": len(tokenized_dataset),\n",
        "    \"trainable_parameters\": trainable_params,\n",
        "    \"total_parameters\": total_params,\n",
        "    \"trainable_percentage\": 100 * trainable_params / total_params,\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "}\n",
        "\n",
        "# Save as JSON\n",
        "with open(\"training_log.json\", \"w\") as f:\n",
        "    json.dump(training_log, f, indent=2)\n",
        "\n",
        "# Save as text\n",
        "with open(\"training_log.txt\", \"w\") as f:\n",
        "    f.write(\"Training Log\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    for key, value in training_log.items():\n",
        "        f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "print(\"✓ Training logs saved to training_log.txt and training_log.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdQkH8Wnnd6j"
      },
      "source": [
        "## 13. Model Evaluation\n",
        "\n",
        "Test the fine-tuned model with diverse prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6Ah3evznd6j",
        "outputId": "8444e251-8572-451d-f6fb-393725497a75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Testing Fine-tuned Model\n",
            "================================================================================\n",
            "\n",
            "--- Test 1 ---\n",
            "Prompt:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Explain what artificial intelligence is in simple terms.\n",
            "\n",
            "### Response:\n",
            "\n",
            "\n",
            "Generated Response:\n",
            "Artificial Intelligence (AI) is a field of study that focuses on creating machines and programs that can perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. In simpler terms, AI involves creating software or systems that can understand, interpret, and respond to information in a way that mimics human intelligence.\n",
            "\n",
            "For example, AI can be used to:\n",
            "1. **Learning from data**: AI systems can analyze large amounts of data and learn patterns or trends over time.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Test 2 ---\n",
            "Prompt:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Write a haiku about the ocean.\n",
            "\n",
            "### Response:\n",
            "\n",
            "\n",
            "Generated Response:\n",
            "The ocean stretches,  \n",
            "Whispers of waves,  \n",
            "A timeless, endless embrace.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Test 3 ---\n",
            "Prompt:\n",
            "Below is an instruction that describes a task, possibly with an input, that needs to be completed. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Classify the sentiment of the following text.\n",
            "\n",
            "### Input:\n",
            "I love this product! It works perfectly.\n",
            "\n",
            "### Response:\n",
            "\n",
            "\n",
            "Generated Response:\n",
            "The sentiment of the text is **Positive**. The phrase \"I love this product! It works perfectly.\" clearly expresses a strong positive sentiment, as it indicates enjoyment and satisfaction with the product.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test prompts (not in training data)\n",
        "test_prompts = [\n",
        "    \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Explain what artificial intelligence is in simple terms.\n",
        "\n",
        "### Response:\n",
        "\"\"\",\n",
        "    \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Write a haiku about the ocean.\n",
        "\n",
        "### Response:\n",
        "\"\"\",\n",
        "    \"\"\"Below is an instruction that describes a task, possibly with an input, that needs to be completed. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Classify the sentiment of the following text.\n",
        "\n",
        "### Input:\n",
        "I love this product! It works perfectly.\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Testing Fine-tuned Model\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n--- Test {i} ---\")\n",
        "    print(f\"Prompt:\\n{prompt}\")\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the generated response (after the prompt)\n",
        "    response = generated_text[len(prompt):]\n",
        "\n",
        "    print(f\"\\nGenerated Response:\\n{response}\")\n",
        "    print(\"\\n\" + \"-\"*80)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
